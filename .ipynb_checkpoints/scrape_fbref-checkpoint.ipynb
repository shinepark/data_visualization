{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be93aa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from functools import reduce\n",
    "import sys\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "\n",
    "def get_data_info():\n",
    "    # all possible leagues and seasons\n",
    "    leagues = [\"FA Women's Super League\"]\n",
    "    seasons = ['2020-2021', '2021-2022', '2022-2023', '2023-2024', '2024-2025']\n",
    "    \n",
    "    while True:\n",
    "        # select league \n",
    "        league = input(\"Select League (FA Women's Super League): \")\n",
    "        \n",
    "        # check if input valid\n",
    "        if league not in leagues:\n",
    "            print('League not valid, try again')\n",
    "            continue\n",
    "            \n",
    "        # assign url names and id's\n",
    "        if league == \"FA Women's Super League\":\n",
    "            league = \"FA-Women's-Super-League\"\n",
    "            league_id = '189'\n",
    "        '''\n",
    "        if league == 'La Liga':\n",
    "            league = 'La-Liga'\n",
    "            league_id = '12'\n",
    "\n",
    "        if league == 'Serie A':\n",
    "            league = 'Serie-A'\n",
    "            league_id = '11'\n",
    "\n",
    "        if league == 'Ligue 1':\n",
    "            league = 'Ligue-1'\n",
    "            league_id = '13'\n",
    "\n",
    "        if league == 'Bundesliga':\n",
    "            league = 'Bundesliga'\n",
    "            league_id = '20'\n",
    "        '''\n",
    "        break\n",
    "            \n",
    "    while True: \n",
    "        # select season \n",
    "        season = input('Select Season (2020-2021, 2021-2022, 2022-2023, 2023-2024, 2024-2025): ')\n",
    "        \n",
    "        # check if input valid\n",
    "        if season not in seasons:\n",
    "            print('Season not valid, try again')\n",
    "            continue\n",
    "        break\n",
    "\n",
    "    url = f'https://fbref.com/en/comps/{league_id}/{season}/schedule/{season}-{league}-Scores-and-Fixtures'\n",
    "    return url, league, season\n",
    "\n",
    "\n",
    "def get_fixture_data(url, league, season):\n",
    "    print('Getting fixture data...')\n",
    "    # create empty data frame and access all tables in url\n",
    "    fixturedata = pd.DataFrame([])\n",
    "    tables = pd.read_html(url)\n",
    "    \n",
    "    # get fixtures\n",
    "    fixtures = tables[0][['Wk', 'Day', 'Date', 'Time', 'Home', 'Away', 'xG', 'xG.1', 'Score']].dropna()\n",
    "    fixtures['season'] = url.split('/')[6]\n",
    "    fixturedata = pd.concat([fixturedata,fixtures])\n",
    "    \n",
    "    # assign id for each game\n",
    "    fixturedata[\"game_id\"] = fixturedata.index\n",
    "    \n",
    "    # export to csv file\n",
    "    fixturedata.reset_index(drop=True).to_csv(f'{league.lower()}_{season.lower()}_fixture_data.csv', \n",
    "        header=True, index=False, mode='w')\n",
    "    print('Fixture data collected...')\n",
    "\n",
    "\n",
    "def get_match_links(url, league):   \n",
    "    print('Getting player data...')\n",
    "    # access and download content from url containing all fixture links    \n",
    "    match_links = []\n",
    "    html = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    links = soup(html.content, \"html.parser\").find_all('a')\n",
    "    \n",
    "    # filter list to return only needed links\n",
    "    key_words_good = ['/en/matches/', f'{league}']\n",
    "    for l in links:\n",
    "        href = l.get('href', '')\n",
    "        if all(x in href for x in key_words_good):\n",
    "            if 'https://fbref.com' + href not in match_links:                 \n",
    "                match_links.append('https://fbref.com' + href)\n",
    "    return match_links\n",
    "\n",
    "\n",
    "def player_data(match_links, league, season):\n",
    "    # loop through all fixtures\n",
    "    player_data = pd.DataFrame([])\n",
    "    for count, link in enumerate(match_links):\n",
    "        try:\n",
    "            tables = pd.read_html(link)\n",
    "            for table in tables:\n",
    "                try:\n",
    "                    table.columns = table.columns.droplevel()\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "            # get player data\n",
    "            def get_team_1_player_data():\n",
    "                # outfield and goal keeper data stored in seperate tables \n",
    "                data_frames = [tables[3], tables[9]]\n",
    "                \n",
    "                # merge outfield and goal keeper data\n",
    "                df = reduce(lambda left, right: pd.merge(left, right, \n",
    "                    on=['Player', 'Nation', 'Age', 'Min'], how='outer'), data_frames).iloc[:-1]\n",
    "                \n",
    "                # assign a home or away value\n",
    "                return df.assign(home=1, game_id=count)\n",
    "\n",
    "            # get second teams  player data        \n",
    "            def get_team_2_player_data():\n",
    "                data_frames = [tables[10], tables[16]]\n",
    "                df = reduce(lambda left, right: pd.merge(left, right,\n",
    "                    on=['Player', 'Nation', 'Age', 'Min'], how='outer'), data_frames).iloc[:-1]\n",
    "                return df.assign(home=0, game_id=count)\n",
    "\n",
    "            # combine both team data and export all match data to csv\n",
    "            t1 = get_team_1_player_data()\n",
    "            t2 = get_team_2_player_data()\n",
    "            player_data = pd.concat([player_data, pd.concat([t1,t2]).reset_index()])\n",
    "            \n",
    "            print(f'{count+1}/{len(match_links)} matches collected')\n",
    "            player_data.to_csv(f'{league.lower()}_{season.lower()}_player_data.csv', \n",
    "                header=True, index=False, mode='w')\n",
    "        except:\n",
    "            print(f'{link}: error')\n",
    "        # sleep for 3 seconds after every game to avoid IP being blocked\n",
    "        time.sleep(3)\n",
    "\n",
    "\n",
    "# main function\n",
    "def main(): \n",
    "    url, league, season = get_data_info()\n",
    "    get_fixture_data(url, league, season)\n",
    "    match_links = get_match_links(url, league)\n",
    "    player_data(match_links, league, season)\n",
    "\n",
    "    # checks if user wants to collect more data\n",
    "    print('Data collected!')\n",
    "    while True:\n",
    "        answer = input('Do you want to collect more data? (yes/no): ')\n",
    "        if answer == 'yes':\n",
    "            main()\n",
    "        if answer == 'no':\n",
    "            sys.exit()\n",
    "        else:\n",
    "            print('Answer not valid')\n",
    "            continue\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        main()\n",
    "    except HTTPError:\n",
    "        print('The website refused access, try again later')\n",
    "        time.sleep(5)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
